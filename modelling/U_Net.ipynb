{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net for Crop Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook investigates the performance of a U-Net model in crop classification using the Zueri Crop dataset. Various configurations of the U-Net model will be explored. The goal is to analyze how the hyperparameters impact the model's accuracy and generalization on crop classification tasks. The notebook utilizes the `DeepModel_Trainer` class for data loading and model training, `UNet_small` and `UNet_Dropout` as modell architecture  and evaluation metrics are logged using WandB for comprehensive analysis.\n",
    "\n",
    "The inputs for the U-Net are upsampled from 24 x 24 pixels to 80 x 80 pixels with the method of bicubic upsampling. The encoder part of the network consists of four downsampling blocks, each composed of two convolutional layers with ReLU activation followed by max-pooling, progressively reducing the spatial dimensions of the input. The encoder also includes dropout layers with a specified dropout rate applied to the fourth downsampling block. The decoder part consists of four upsampling blocks, each involving a transpose convolution operation and two convolutional layers with ReLU activation. Skip connections concatenate feature maps from the encoder to the decoder, helping preserve spatial information. The final output is produced by a 1x1 convolutional layer to generate segmentation masks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import wandb\n",
    "from sklearn.model_selection import KFold\n",
    "from IPython.display import IFrame\n",
    "from src.dataset import Dataset\n",
    "from src.modelling import DeepModel_Trainer\n",
    "from models.UNet_small import UNet_small\n",
    "from models.UNet_Dropout import UNet\n",
    "from torcheval.metrics.functional import multiclass_f1_score\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdaniela-herzig\u001b[0m (\u001b[33mdlbs_crop\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /users/dherzig/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key = '***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "For overfitting the Trainer_loader consists only of one batch. The model should learn the batch fast and should decrease the train loss substantially and the test loss should decrease because of the overftting and the missing generalization of the single batch for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_small=UNet_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit = DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_small)\n",
    "overfit.train_model('Overfitting', 'Unet_small', 'all', num_epochs=300, test_model=True, lr=5e-4, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First run unregularized\n",
    "\n",
    "Because of the experience of the Challenge X, we runned a first model with an augmentation rate of 0.66 and no regularization, this were the default values from the ETHZ Paper described. It is a way of a baseline for the U-Net to get a feeling for the next regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_run = DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_small)\n",
    "first_run.train_model('Run-unregularized', 'Unet_small', 'all', num_epochs=30, test_model=False, lr=1e-3, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularize\n",
    "## Dropout\n",
    "Different dropout rates are implemented in the fourth downsampling block and in the long-format downsampling block in the middle of the U-Net network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_dropout = UNet(rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_05 = DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_dropout)\n",
    "Dropout_05.train_model('Run-dropout_0.5', 'Unet_dropout', 'all', num_epochs=30, test_model=False, lr=1e-3, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_dropout = UNet(rate=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout_03 = DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_dropout)\n",
    "Dropout_03.train_model('Run-dropout_0.3', 'Unet_dropout', 'all', num_epochs=30, test_model=False, lr=1e-3, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight decay\n",
    "Different weight_decays are implemented for the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Unet_small=UNet_small()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay= DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_small)\n",
    "weight_decay.train_model('Run-weightDecay_1e-4', 'Unet_small', 'all', num_epochs=30, test_model=False, lr=1e-3, batch_size=4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay.train_model('Run-weightDecay_1e-3', 'Unet_small', 'all', num_epochs=30, test_model=False, lr=1e-3, batch_size=4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batchsize\n",
    "\n",
    "For tuning, the batch size is changed to 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsize= DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_small)\n",
    "batchsize.train_model('Run-batchsize-16', 'Unet_small', 'all', num_epochs=30, test_model=False, lr=5e-4, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation Rate\n",
    "\n",
    "The augmentation rate is set to 0.33. The default augmentation rate of 0.66 was first run as a kind of baseline for the UNet (First run unregularized, see above) based on our Challenge X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment= DeepModel_Trainer('../scratch/ZueriCrop/ZueriCrop.hdf5', 'labels.csv', Unet_small, augment_rate=0.33)\n",
    "augment.train_model('Run-augmentation-0.33', 'Unet_small', 'all', num_epochs=30, test_model=False, lr=1e-3, batch_size=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KernelAgro",
   "language": "python",
   "name": "kernelagro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
